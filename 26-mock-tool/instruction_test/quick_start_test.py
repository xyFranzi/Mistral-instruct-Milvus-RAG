# è¿è¡Œè¿™ä¸ªç®€åŒ–ç‰ˆæœ¬å¿«é€Ÿå¼€å§‹

from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, SystemMessage

def quick_test():
    llm = ChatOllama(
        model="mistral:7b-instruct-v0.3-q5_0",
        temperature=0,
    )
    
    # å¿«é€Ÿæµ‹è¯• - åªæµ‹è¯•å‡ ä¸ªå…³é”®æ¡ˆä¾‹
    system_prompt = """Du hast zwei Werkzeuge:
1. SimilaritySearch: fÃ¼r konzeptuelle Fragen Ã¼ber AI/ML
2. QA: fÃ¼r spezifische Anweisungen und Fakten

Antworte nur mit "SimilaritySearch" oder "QA"."""

    test_cases = [
        ("Was ist kÃ¼nstliche Intelligenz?", "SimilaritySearch"),
        ("Wie installiere ich Python?", "QA"),
        ("Was ist Deep Learning?", "SimilaritySearch"),
        ("Wo finde ich die API-Dokumentation?", "QA"),
    ]
    
    print("å¿«é€Ÿæµ‹è¯•ç»“æœ:")
    print("-" * 40)
    
    correct = 0
    for question, expected in test_cases:
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=question)
        ]
        
        response = llm.invoke(messages)
        actual = response.content.strip().lstrip()
        
        is_correct = actual == expected
        if is_correct:
            correct += 1
            
        status = "âœ…" if is_correct else "âŒ"
        print(f"{status} {question}")
        print(f"   æœŸæœ›: {expected}")
        print(f"   å®é™…: '{actual}'")
        print(f"   Token: {response.usage_metadata.get('total_tokens', 'N/A')}")
        print()
    
    accuracy = correct / len(test_cases)
    print(f"å‡†ç¡®ç‡: {accuracy:.2%} ({correct}/{len(test_cases)})")
    
    if accuracy >= 0.75:
        print("ğŸ‰ åˆæ­¥æµ‹è¯•é€šè¿‡ï¼å¯ä»¥ç»§ç»­æ·±å…¥æµ‹è¯•")
    else:
        print("âš ï¸  éœ€è¦è°ƒæ•´promptæˆ–è€ƒè™‘å…¶ä»–æ¨¡å‹")

if __name__ == "__main__":
    quick_test()